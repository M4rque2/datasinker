# datasinker
================================== 

Transfer data from msg queue to database 

这个库用来将爬虫推送到Kafka队列中的数据写入SQL兼容数据库（在我们的使用场景里主要是TiDB）  

你可能会问几个问题：  

- 为什么爬虫要将消息推送到Kafka而不是直接写入数据库？  
首先是安全隔离，运行爬虫的机器上可能有各种奇技淫巧，在爬虫运行的机器上设置强安全策略是非常不方便的，数据库里可能还有其他高价值的数据资产，因此有必要通过Kafka集群做安全隔离。  
其次是性能问题，大规模分布式爬虫可以有很恐怖的数据流量，数据库集群不一定有扛得住，这种场景Kafka非常合适。实测TiDB集群连接不稳定的情况还是经常发生，这时需要一个地方把数据暂时落地，等待数据库集群恢复，再重新写入。这种写入策略，如果放到分布式爬虫的pipeline里（scrapy的逻辑），考虑到分布式的影响，逻辑和算法都很复杂，通过一个data sinker程序统一处理就简单的多。
我们在长期的开发中摸索出来的最佳实践是，通过Kafka Topic指定数据库，通过Kafka message key指定表名，Kafka message value值固定为JSON格式，JSON key对应列名，value入库。
这种方式简单高效，长期实践中运行良好。  

- 为什么选择Dataset作为ORM库？  
主流的ORM库有peewee，SQLAlchemy，Django的ORM。简而言之，爬虫数据入库的实际需求和后端的ORM有很大的不同。后端常说CRUD四大操作，爬虫入库的时候C（自动建表）有待商榷，R和D肯定没有，其实主要需求是U。那么后端的ORM库大多时候太重了，大多数时候他们都会建一个类映射数据库的DDL语句。dataset在SQLAlchemy的基础上简单封装，以JSON格式本位入库，正好契合爬虫的需求。
