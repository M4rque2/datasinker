# datasinker
Transfer data from msg queue to database
这个库用来将爬虫推送到Kafka队列中的数据写入SQL兼容数据（在我们的使用场景里主要是TiDB）
你可能会问几个问题：
为什么爬虫要将消息推送到Kafka而不是直接写入数据库？
首先是安全隔离，运行爬虫的机器上可能有各种奇技淫巧，在爬虫运行的机器上设置强安全策略是非常不方便的，数据库里可能还有其他高价值的数据资产，因此有必要通过Kafka集群做安全隔离。
其次是性能问题，大规模分布式爬虫可以有很恐怖的数据流量，数据库集群不一定有扛得住，这种场景Kafka非常合适。实测TiDB集群连接不稳定的情况还是经常发生，这时需要一个地方把数据暂时落地，等待数据库集群恢复，再重新写入。这种写入策略，如果放到分布式爬虫的pipeline里（scrapy的逻辑），考虑到分布式的影响，逻辑和算法都很复杂，通过一个data sinker程序统一处理就简单的多。

我们在长期的开发中摸索出来的最佳实践是，通过Kafka Topic指定数据库，通过Kafka message key指定表名，Kafka message value值固定为JSON格式，JSON key对应列名，value入库。
这种方式简单高效，长期实践中运行良好。
